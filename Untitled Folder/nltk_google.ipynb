{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from langdetect) (1.12.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\ec\\0c\\a9\\1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import nltk\n",
    "import time\n",
    "from openpyxl import load_workbook\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #감정분석\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize #토큰 생성\n",
    "from nltk.tag import pos_tag # 형태소 태그생성\n",
    "from nltk import FreqDist #빈도수 측정\n",
    "##from wordcloud import WordCloud #워드 클라우드\n",
    "import matplotlib.pyplot as plt\n",
    "import re #정규식 사용\n",
    "from datetime import datetime, timedelta #시간계산 datetime(날짜), time,delta(시간의 차)\n",
    "from pytz import timezone #시간대 변경\n",
    "from googletrans import Translator#구글 번역\n",
    "from langdetect import detect#언어감지\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from json import JSONDecodeError\n",
    "\n",
    "nltk.download(\"book\")\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "endtype = {1 : \"success\", 2:'fail', 3:'no change'}\n",
    "\n",
    "def open_workbook(file = None): \n",
    "  workbook = load_workbook(file, data_only=True)#엑셀 열기\n",
    "  return workbook\n",
    "\n",
    "def clean_str(text):\n",
    "  try:\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)  #string에서 pattern과 매치하는 텍스트를 repl로 치환한다\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = re.compile(re.escape('(Translated by Google)')+'.*') # 구글 뒤 처리\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = re.compile(re.escape('(Google 번역)')+'.*') # 구글 뒤 처리\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    ##패턴 문자열 pattern을 패턴 객체로 컴파일한다\n",
    "    pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #text = re.compile(re.escape('((Google 번역))')+'.*')\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    #text = re.compile(re.escape('(Translated by Google)')+'.*')\n",
    "    #text = re.sub(pattern=pattern, repl='', string=text)\n",
    "  except TypeError:\n",
    "    test = 'neutral'\n",
    "  return text     \n",
    "\n",
    "def remwithre(text, there=re.compile(re.escape('\\n'))):\n",
    "    return there.sub('', text)\n",
    "def remwithre2(text, there=re.compile(re.escape('(Translated by Google)')+'.*')):\n",
    "    return there.sub('', text)\n",
    "def remwithre3(text, there=re.compile(re.escape('(Google 번역)')+'.*')):\n",
    "    return there.sub('', text)\n",
    "\n",
    "def get_comments(sheet,startrow=1) :\n",
    "  result = []\n",
    "  row_num = sheet.max_row#시트 줄수\n",
    "  col_num = sheet.max_column#시트 열수\n",
    "\n",
    "  if row_num<startrow:\n",
    "    return result\n",
    "\n",
    "  for r in range(startrow, row_num+1) :\n",
    "    clean_comment = clean_str(sheet.cell(r,2).value)\n",
    "    clean_comment = remwithre(clean_comment)\n",
    "    clean_comment = remwithre2(clean_comment)\n",
    "    clean_comment = remwithre3(clean_comment)\n",
    "    if clean_comment is None: #empty set\n",
    "      clean_comment = 'neutral'\n",
    "    result.append(clean_comment)\n",
    "\n",
    "  return result\n",
    "\n",
    "def get_translate(comments = None):\n",
    "  result = []\n",
    "  for i in range(0,len(comments)) :\n",
    "    try:\n",
    "      translator = Translator(service_urls=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.co.kr',\n",
    "      'translate.google.co.jp',\n",
    "      'translate.google.co.uk',\n",
    "      ])\n",
    "\n",
    "      #lang = detect(comments[i])\n",
    "      #print(i,translator.detect(comments[i]).lang, comments[i])\n",
    "      if(translator.detect(comments[i]).lang == 'en'):\n",
    "        result.append(comments[i])\n",
    "      else:\n",
    "        time.sleep(1)\n",
    "        trs_comment = translator.translate(comments[i])\n",
    "        print(comments[i], \"\\ntranslated : \",trs_comment.text)\n",
    "        result.append(trs_comment.text)\n",
    "    except (LangDetectException, AttributeError, TypeError):\n",
    "      result.append('neutral')\n",
    "    except JSONDecodeError:\n",
    "      print(\"JSONERROR :\", comments[i])\n",
    "      result.append(comments[i])\n",
    "  \n",
    "  return result\n",
    "\n",
    "def analyze_emotion(trans_list = None) :#점수 매기기\n",
    "  result = []\n",
    "  for i in trans_list:\n",
    "    score = sid.polarity_scores(i)\n",
    "    result.append(score)\n",
    "    \n",
    "  return result\n",
    "\n",
    "\n",
    "def get_like(sheet = None, startrow = 1) : #int + 답변의 내용에서 좋아요 숫자를 구하기(정규표현식)\n",
    "  desc = re.compile('\\d*\\d')\n",
    "  result = []\n",
    "\n",
    "  for i in range(startrow,sheet.max_row+1) :\n",
    "    find_like = desc.findall(sheet.cell(i,4).value)\n",
    "\n",
    "    if len(find_like) is None:\n",
    "      result.append('0')\n",
    "    else :\n",
    "      #print(find_like)\n",
    "      if len(find_like) == 1 :\n",
    "        result.append('0')\n",
    "      elif len(find_like) == 2:\n",
    "        #print(int(find_like[1])-1)\n",
    "        result.append(str(int(find_like[1])-1))            \n",
    "  return result\n",
    "\n",
    "\n",
    "\n",
    "def get_realtime(sheet = None, startrow = 1):#시간계산\n",
    "  #년,달,주,일,시간,분\n",
    "\n",
    "  minute_desc = re.compile('\\d*\\d분')\n",
    "  hour_desc = re.compile('\\d*\\d시간')\n",
    "  day_desc = re.compile('\\d*\\d일')\n",
    "  week_desc = re.compile('\\d*\\d주')\n",
    "  month_desc = re.compile('\\d*\\개월')\n",
    "  year_desc = re.compile('\\d*\\d년')\n",
    "  desc = re.compile('\\d*\\d')\n",
    "\n",
    "  #시간 출력 포맷\n",
    "  format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "  #현재시간\n",
    "  UTC = datetime.now(timezone('UTC'))\n",
    "  KST = datetime.now(timezone('Asia/Seoul'))\n",
    "\n",
    "  #print(KST)\n",
    "\n",
    "  result = []\n",
    "  \n",
    "  # ~전 토큰으로 자름 return객체 = list\n",
    "    \n",
    "  for i in range(startrow,sheet.max_row+1) :\n",
    "    minute_ago = minute_desc.findall(sheet.cell(i,2).value)\n",
    "    hour_ago = hour_desc.findall(sheet.cell(i,2).value)\n",
    "    day_ago = day_desc.findall(sheet.cell(i,2).value)\n",
    "    week_ago = week_desc.findall(sheet.cell(i,2).value)\n",
    "    month_ago = month_desc.findall(sheet.cell(i,2).value)\n",
    "    year_ago = year_desc.findall(sheet.cell(i,2).value)\n",
    "    #print(minute_ago,hour_ago,day_ago,week_ago,month_ago,year_ago)\n",
    "    \n",
    "    # 길이 != 0 -> 현재시간 - 차이 시간 = return객체 = datetime -> str으로 변경\n",
    "    if len(minute_ago) != 0:\n",
    "      min_out = int((desc.findall(minute_ago[0]))[0])\n",
    "      rtime = KST-timedelta(minutes = min_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "      \n",
    "    elif len(hour_ago) != 0:\n",
    "      hour_out = int((desc.findall(hour_ago[0]))[0])\n",
    "      rtime = KST-timedelta(hours = hour_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "      \n",
    "    elif len(day_ago) != 0:\n",
    "      day_out = int((desc.findall(day_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = day_out)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(week_ago) != 0:\n",
    "      week_out = int((desc.findall(week_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = week_out*7)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(month_ago) != 0:\n",
    "      month_out = int((desc.findall(month_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = month_out*30)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "\n",
    "    elif len(year_ago) != 0:\n",
    "      year_out = int((desc.findall(year_ago[0]))[0])\n",
    "      rtime = KST-timedelta(days = year_out*365)\n",
    "      str_rtime = rtime.strftime(format)\n",
    "      #print(str_rtime)\n",
    "      result.append(str_rtime)\n",
    "    \n",
    "    else :\n",
    "      result.append(KST.strftime(format))\n",
    "\n",
    "  return result\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def add_score2excel(file = None,sheet = None,startrow=1): #파일명, 시작row\n",
    "  if file is None and sheet is None:\n",
    "    return endtype[2]\n",
    "  if startrow < 1 :\n",
    "    startrow = 1\n",
    "  \n",
    "  book = open_workbook(file)\n",
    "  sheet1 = book[sheet]\n",
    "  \n",
    "  comment_list = get_comments(sheet1,startrow)\n",
    "  trans_list = get_translate(comment_list)# 번역된 리스트\n",
    "  score_list = analyze_emotion(trans_list) # 점수 리스트\n",
    "  #like_list = get_like(sheet1,startrow) # like 수 리스트\n",
    "  #time_list = get_realtime(sheet1,startrow) # 절대 시간 리스트\n",
    "\n",
    "  #print(len(comment_list),len(trans_list),len(score_list),len(like_list),len(time_list))\n",
    "\n",
    "  if(len(score_list)==0):\n",
    "      return endtype[3]\n",
    "  sheet1_row = sheet1.max_row\n",
    "\n",
    "  writerow = startrow#점수 열에 추가하기 D E F G\n",
    "  for i in range(0,len(score_list)):\n",
    "    #print(writerow ,score_list[i])\n",
    "    #sheet1.cell(writerow,2).value = time_list[i]\n",
    "    sheet1.cell(writerow,2).value = trans_list[i] #번역\n",
    "    #sheet1.cell(writerow,5).value = like_list[i]\n",
    "    sheet1.cell(writerow,4).value = score_list[i]['neg'] #부정\n",
    "    sheet1.cell(writerow,5).value = score_list[i]['neu'] #중립\n",
    "    sheet1.cell(writerow,6).value = score_list[i]['pos'] #긍정\n",
    "    sheet1.cell(writerow,7).value = score_list[i]['compound'] #복합\n",
    "    writerow+=1\n",
    "    #emotion_list = [i,[score_list[i]['neg'], score_list[i]['neu'], score_list[i]['pos'], score_list[i]['compound']]]\n",
    "    #print(writerow ,score_list[i])\n",
    "    \n",
    "    \n",
    "  book.save(file)\n",
    "  return endtype[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-946f723a493d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#print(add_score2excel(file2,sheet4,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#print(add_score2excel(file2,sheet5,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_score2excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msheet11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;31m#print(add_score2excel(file2,sheet2,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#print(add_score2excel(file2,sheet1,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b97570e5dba2>\u001b[0m in \u001b[0;36madd_score2excel\u001b[1;34m(file, sheet, startrow)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0msheet1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[0mcomment_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m   \u001b[0mtrans_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 번역된 리스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m   \u001b[0mscore_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_emotion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 점수 리스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b97570e5dba2>\u001b[0m in \u001b[0;36mget_comments\u001b[1;34m(sheet, startrow)\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_num\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mclean_comment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mclean_comment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremwithre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mclean_comment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremwithre2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mclean_comment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremwithre3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-b97570e5dba2>\u001b[0m in \u001b[0;36mremwithre\u001b[1;34m(text, there)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremwithre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mthere\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremwithre2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(Translated by Google)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mthere\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "#testline\n",
    "\n",
    "#file = \"C:\\datasource\\Samsung.xlsx\"\n",
    "#file = \"googlemaps_reply2.xlsx\"\n",
    "file2 = \"googlemaps_reply2.xlsx\"\n",
    "\n",
    "#sheet3 = 'b3갓바위'\n",
    "#sheet4 = 'b4앞산공원'\n",
    "#sheet5 = 'b5파계사'\n",
    "sheet11 = 'w10국립대구박물관'\n",
    "#sheet2 = '2'\n",
    "#sheet1 = '1'\n",
    "#filename = \"TripAdvisor_reply.xlsx\"\n",
    "#book = openpyxl.load_workbook(filename)\n",
    "\n",
    "book = open_workbook(file2)\n",
    "\n",
    "#sheet1 = book[sheet]#\n",
    "\n",
    "#############################################\n",
    "\n",
    "#print(add_score2excel(file,sheet,1))\n",
    "\n",
    "#print(add_score2excel(file2,sheet3,1))\n",
    "#print(add_score2excel(file2,sheet4,1))\n",
    "#print(add_score2excel(file2,sheet5,1))\n",
    "print(add_score2excel(file2,sheet11,1))\n",
    "#print(add_score2excel(file2,sheet2,1))\n",
    "#print(add_score2excel(file2,sheet1,1))\n",
    "\n",
    "#noun_list = make_wcloud(file,'Sheet1')\n",
    "#noun_list = make_wcloud(file2,'Sheet1')\n",
    "\n",
    "############################################\n",
    "\n",
    "#result = get_comments(sheet1,1070)\n",
    "#print(result,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
